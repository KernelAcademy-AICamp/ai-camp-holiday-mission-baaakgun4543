{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./res/rag_data.pkl', 'rb') as f:\n",
    "    rag_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rdk00nsryre",
   "source": "## 📂 RAG 데이터 불러오기\n\n이전에 `rag_data.ipynb`에서 저장한 Pickle 파일을 불러옵니다.\n\n- **`pickle.load()`**: 저장된 파이썬 객체를 메모리로 다시 불러오기\n- **`'rb'` 모드**: Read Binary (바이너리 읽기 모드)\n- **`rag_data`**: 질문, 컨텍스트, 정답 등이 담긴 딕셔너리\n- **목적**: RAG 시스템의 효과를 실험하기 위한 데이터 준비",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = rag_data['questions'][:10]\n",
    "contexts = rag_data['contexts'][:10]\n",
    "answers = rag_data['answers'][:10]\n",
    "len(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8x9cxdl9hag",
   "source": "## 📊 테스트 데이터 준비\n\n실험을 위해 처음 10개의 데이터만 추출합니다.\n\n- **`questions`**: 10개의 질문 리스트\n- **`contexts`**: 각 질문마다 3개씩의 컨텍스트 (총 10x3 구조)\n- **`answers`**: 각 질문에 대한 정답 10개\n- **`[:10]`**: 파이썬 슬라이싱으로 처음 10개만 선택\n- **`len(questions)`**: 추출한 질문 개수 확인 (결과: 10)\n\n### 목적\n빠른 실험을 위해 전체 200개가 아닌 일부만 사용합니다.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from utils import call_openai, get_embeddings, cosine_similarity\n",
    "\n",
    "predictions = []\n",
    "for i in tqdm(range(len(questions))):\n",
    "    prompt = f\"\"\"You are an expert in Finance. Please answer to the question given below.\n",
    "    \n",
    "Question:\n",
    "{questions[i]}\n",
    "\"\"\"\n",
    "\n",
    "    prediction = call_openai(prompt, model='gpt-4o-2024-05-13')\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aj6odrpy06",
   "source": "## 🤖 RAG 없이 GPT-4로 직접 답변 생성\n\n**컨텍스트를 제공하지 않고** GPT-4에게 질문만 던져서 답변을 받습니다.\n\n### 동작 과정\n1. **반복문**: 10개 질문을 하나씩 처리\n2. **프롬프트 작성**: \n   - \"당신은 금융 전문가입니다\"\n   - 질문만 제공 (참고할 문서 없음)\n3. **`call_openai()`**: GPT-4 모델에게 답변 요청\n4. **결과 저장**: 생성된 답변을 `predictions` 리스트에 추가\n\n### 핵심 포인트 ⚠️\n- **컨텍스트 없음**: AI가 자신의 학습 데이터만으로 답변\n- **한계**: 최신 정보나 특정 문서 내용은 모를 수 있음\n- **비교 기준**: 나중에 RAG를 적용한 결과와 비교할 기준점\n\n### 결과\n10개 질문에 대한 GPT-4의 직접 답변이 `predictions`에 저장됩니다.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b33e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness\n",
    "\n",
    "\n",
    "data_samples = {\n",
    "    'question': questions,\n",
    "    'answer': predictions,\n",
    "    'ground_truth': answers\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "score = evaluate(dataset, metrics=[answer_correctness])\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hoo9y7h944",
   "source": "## 📉 RAG 없는 답변의 성능 평가\n\n컨텍스트 없이 생성한 답변의 정확도를 평가합니다.\n\n### 평가 데이터 구성\n- **`question`**: 사용자 질문 10개\n- **`answer`**: GPT-4가 생성한 답변 (컨텍스트 없음)\n- **`ground_truth`**: 실제 정답\n\n### 평가 메트릭\n- **`answer_correctness`**: 답변이 정답과 얼마나 일치하는지 측정\n  - 0.0 ~ 1.0 사이 값\n  - 1.0에 가까울수록 정확한 답변\n  - **의미적 유사도 + 사실 정확도** 모두 고려\n\n### 이 점수의 의미\n- **기준점(Baseline)**: RAG를 적용하지 않은 상태의 성능\n- **비교 대상**: 다음 단계에서 RAG를 적용한 후의 성능과 비교\n- **예상 결과**: RAG를 적용하면 이 점수보다 높아질 것으로 기대",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jy3wsdh3u4",
   "source": "## 🔍 첫 번째 질문 확인\n\n테스트 데이터의 첫 번째 질문을 출력합니다.\n\n- **목적**: 어떤 질문에 대해 평가하고 있는지 확인\n- **예시**: \"글로벌 저금리 현상이 부각된 원인은 무엇인가요?\"\n- **다음 셀과 연결**: 이 질문에 대한 GPT-4의 답변을 확인할 예정",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e715025",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mo79dcb0ft",
   "source": "## 💬 RAG 없는 답변 확인\n\n컨텍스트 없이 GPT-4가 생성한 첫 번째 답변을 출력합니다.\n\n### 특징\n- **컨텍스트 없음**: 특정 문서를 참고하지 않고 생성\n- **일반적인 답변**: GPT-4의 학습 데이터에 기반한 일반적 지식\n- **한계점**: \n  - 구체적인 세부사항이 부족할 수 있음\n  - 최신 정보가 반영되지 않을 수 있음\n  - 특정 문서의 정확한 내용과 다를 수 있음\n\n### 비교 준비\n다음 단계에서 **RAG를 적용한 답변**과 비교하여 얼마나 개선되는지 확인할 예정입니다.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e4939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import call_openai, get_embeddings, cosine_similarity\n",
    "\n",
    "\n",
    "def retrieve_context(question, contexts):\n",
    "    question_embedding = get_embeddings([question], model='text-embedding-3-large')[0]\n",
    "    context_embeddings = get_embeddings(contexts, model='text-embedding-3-large')\n",
    "\n",
    "    similarities = [cosine_similarity(question_embedding, context_embedding) for context_embedding in context_embeddings]\n",
    "\n",
    "    most_relevant_index = np.argmax(similarities)\n",
    "    return contexts[most_relevant_index]\n",
    "\n",
    "predictions = []\n",
    "for i in tqdm(range(len(questions))):\n",
    "    context = retrieve_context(questions[i], contexts[i])\n",
    "    prompt = f\"\"\"You are an expert in Finance. Please answer to the question given below. Use information given in Context appropriately.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{questions[i]}\n",
    "\"\"\"\n",
    "    prediction = call_openai(prompt, model='gpt-4o-2024-05-13')\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "erj6c4ajld5",
   "source": "## 🎯 RAG 시스템 구현 및 답변 생성\n\n**컨텍스트를 제공하여** GPT-4가 더 정확한 답변을 생성하도록 합니다.\n\n### `retrieve_context()` 함수 - RAG의 핵심!\n```python\ndef retrieve_context(question, contexts):\n    # 1단계: 질문을 임베딩으로 변환\n    question_embedding = get_embeddings([question], model='text-embedding-3-large')[0]\n    \n    # 2단계: 모든 컨텍스트를 임베딩으로 변환\n    context_embeddings = get_embeddings(contexts, model='text-embedding-3-large')\n    \n    # 3단계: 질문과 각 컨텍스트 간 유사도 계산\n    similarities = [cosine_similarity(question_embedding, ce) for ce in context_embeddings]\n    \n    # 4단계: 가장 유사도가 높은 컨텍스트 찾기\n    most_relevant_index = np.argmax(similarities)\n    \n    # 5단계: 가장 관련 있는 컨텍스트 반환\n    return contexts[most_relevant_index]\n```\n\n### 전체 RAG 프로세스\n1. **검색(Retrieve)**: 질문과 가장 관련 있는 컨텍스트 찾기\n2. **증강(Augment)**: 찾은 컨텍스트를 프롬프트에 추가\n3. **생성(Generate)**: 컨텍스트를 참고하여 GPT-4가 답변 생성\n\n### 프롬프트 차이\n- **이전**: \"질문에 답하세요\"\n- **현재**: \"Context를 적절히 활용하여 질문에 답하세요\"\n\n### 기대 효과 ✨\n- 특정 문서의 정확한 정보 활용\n- 구체적이고 상세한 답변\n- 사실 기반의 정확한 답변",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f4774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness, context_relevancy, context_recall, context_precision\n",
    "\n",
    "\n",
    "data_samples = {\n",
    "    'question': questions,\n",
    "    'answer': predictions,\n",
    "    'ground_truth': answers\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "score = evaluate(dataset, metrics=[answer_correctness])\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kpj82hnq9zo",
   "source": "## 📈 RAG 적용 후 성능 평가\n\n컨텍스트를 제공한 후의 답변 정확도를 평가합니다.\n\n### 평가 데이터 구성\n- **`question`**: 동일한 10개 질문\n- **`answer`**: RAG를 적용하여 생성한 답변 (컨텍스트 있음)\n- **`ground_truth`**: 동일한 정답\n\n### 평가 메트릭\n- **`answer_correctness`**: 답변의 정확도 측정\n\n### 비교 분석 🔍\n| 항목 | RAG 없음 | RAG 적용 |\n|------|---------|---------|\n| 컨텍스트 | ❌ 없음 | ✅ 있음 |\n| 정보 출처 | 학습 데이터 | 특정 문서 |\n| 정확도 | 낮음 | **높음** |\n\n### 기대 결과\n- RAG 없는 경우보다 **점수가 높아질 것으로 예상**\n- 컨텍스트 제공으로 인한 답변 품질 향상 확인\n- **RAG의 효과를 정량적으로 증명**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pqxo6u1kred",
   "source": "## ✨ RAG 적용 답변 확인\n\n컨텍스트를 활용하여 생성한 첫 번째 답변을 출력합니다.\n\n### 특징\n- **컨텍스트 있음**: 관련 문서를 참고하여 생성\n- **구체적인 답변**: 특정 문서의 정보 기반\n- **개선점**:\n  - 더 구체적이고 상세한 내용\n  - 문서에 기반한 정확한 정보\n  - 사실 관계가 명확함\n\n### 비교해볼 점 🔍\n이전 셀(RAG 없는 답변)과 비교하여:\n- 답변의 구체성이 향상되었는가?\n- 특정 문서의 정보가 반영되었는가?\n- 정확도가 개선되었는가?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe7779",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1mx2ljsp48",
   "source": "## ✅ 실제 정답 확인\n\n첫 번째 질문의 실제 정답(ground truth)을 출력합니다.\n\n### 목적\n- **최종 비교**: RAG 없는 답변 vs RAG 적용 답변 vs 실제 정답\n- **평가 기준**: RAGAS가 이 정답을 기준으로 점수를 매김\n- **품질 확인**: 생성된 답변이 실제 정답과 얼마나 유사한지 육안으로 확인\n\n### 3가지 답변 비교 요약\n\n| 구분 | 특징 | 정확도 |\n|------|------|--------|\n| **RAG 없음** | 일반적 지식 | 낮음 |\n| **RAG 적용** | 문서 기반 | **높음** ⬆️ |\n| **실제 정답** | 기준값 | 100% |\n\n### 핵심 인사이트 💡\nRAG를 적용하면 실제 정답에 더 가까운 답변을 생성할 수 있습니다!",
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}