{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d127229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./res/rag_data.pkl', 'rb') as f:\n",
    "    rag_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9y1p3z65ir",
   "source": "## 📂 RAG 데이터 불러오기\n\n이전에 저장한 Pickle 파일을 불러와서 메모리에 로드합니다.\n\n- **`pickle.load()`**: 저장된 파이썬 객체를 다시 불러오기\n- **`'rb'` 모드**: Read Binary (바이너리 읽기 모드)\n- **`rag_data`**: 질문, 컨텍스트, 정답 등이 담긴 딕셔너리\n- **목적**: RAG 시스템의 성능을 평가하기 위한 데이터 준비",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba2df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_data['questions'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037jlornxprg",
   "source": "## 🔍 첫 번째 질문 확인\n\n데이터의 첫 번째 질문을 출력하여 데이터 구조를 확인합니다.\n\n- **`rag_data['questions'][0]`**: 질문 리스트의 첫 번째 요소\n- **예시**: \"글로벌 저금리 현상이 부각된 원인은 무엇인가요?\"\n- **목적**: 어떤 형태의 질문이 들어있는지 확인",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd679192",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_data['contexts'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3uu3uxyug",
   "source": "## 📄 첫 번째 질문의 컨텍스트들 확인\n\n첫 번째 질문에 대한 3개의 후보 컨텍스트(문단)를 출력합니다.\n\n- **`rag_data['contexts'][0]`**: 첫 번째 질문의 컨텍스트 리스트\n- **3개의 문단**: 이 중 1개가 정답을 포함하고 있음\n- **RAG의 핵심**: 이 3개 중에서 질문과 가장 관련 있는 문단을 찾는 것이 목표",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae612f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_embedding, cosine_similarity\n",
    "\n",
    "\n",
    "embed_q = get_embedding(rag_data['questions'][0])\n",
    "embed_c0 = get_embedding(rag_data['contexts'][0][0])\n",
    "embed_c1 = get_embedding(rag_data['contexts'][0][1])\n",
    "embed_c2 = get_embedding(rag_data['contexts'][0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ewwg5bwva2",
   "source": "## 🧮 임베딩 생성\n\n질문과 3개의 컨텍스트를 각각 벡터(숫자 배열)로 변환합니다.\n\n- **`get_embedding()`**: 텍스트를 고차원 벡터로 변환하는 함수\n- **임베딩이란?**: 텍스트를 컴퓨터가 이해할 수 있는 숫자 배열로 표현\n- **목적**: 의미가 비슷한 텍스트는 비슷한 벡터 값을 갖게 됨\n- **변환 결과**: \n  - `embed_q`: 질문의 벡터\n  - `embed_c0`, `embed_c1`, `embed_c2`: 각 컨텍스트의 벡터",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_similarity(embed_q, embed_c0))\n",
    "print(cosine_similarity(embed_q, embed_c1))\n",
    "print(cosine_similarity(embed_q, embed_c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpasilbeihg",
   "source": "## 📊 코사인 유사도 계산\n\n질문과 각 컨텍스트 간의 유사도를 계산합니다.\n\n- **`cosine_similarity()`**: 두 벡터 간의 유사도를 0~1 사이 값으로 계산\n- **코사인 유사도란?**: 두 벡터의 방향이 얼마나 비슷한지 측정 (1에 가까울수록 유사)\n- **출력 결과**: \n  - 첫 번째 컨텍스트와의 유사도\n  - 두 번째 컨텍스트와의 유사도\n  - 세 번째 컨텍스트와의 유사도\n- **해석**: 가장 높은 값을 가진 컨텍스트가 질문과 가장 관련이 깊음",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04728a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_data['contexts_answer_idx'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skctuqmurm",
   "source": "## ✅ 정답 컨텍스트 인덱스 확인\n\n첫 번째 질문의 실제 정답이 어느 컨텍스트에 있는지 확인합니다.\n\n- **`contexts_answer_idx[0]`**: 정답이 들어있는 컨텍스트의 위치 (0, 1, 또는 2)\n- **목적**: 예측 결과와 비교하기 위한 정답 확인\n- **예시**: 만약 값이 2라면, 세 번째 컨텍스트(`contexts[0][2]`)가 정답",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf66c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_q = get_embedding(rag_data['questions'][0], model='text-embedding-3-large')\n",
    "embed_c0 = get_embedding(rag_data['contexts'][0][0], model='text-embedding-3-large')\n",
    "embed_c1 = get_embedding(rag_data['contexts'][0][1], model='text-embedding-3-large')\n",
    "embed_c2 = get_embedding(rag_data['contexts'][0][2], model='text-embedding-3-large')\n",
    "\n",
    "print(cosine_similarity(embed_q, embed_c0))\n",
    "print(cosine_similarity(embed_q, embed_c1))\n",
    "print(cosine_similarity(embed_q, embed_c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nb0aafdv6vi",
   "source": "## 🚀 더 큰 임베딩 모델 테스트\n\n더 성능이 좋은 임베딩 모델로 같은 작업을 수행합니다.\n\n- **`text-embedding-3-large`**: OpenAI의 더 크고 정확한 임베딩 모델\n- **기본 모델 vs Large 모델**: \n  - 기본: 빠르지만 정확도가 낮음\n  - Large: 느리지만 더 정확한 의미 파악\n- **비교 목적**: 모델 크기가 성능에 어떤 영향을 주는지 확인\n- **결과 예상**: Large 모델이 정답 컨텍스트에 더 높은 유사도를 줄 가능성이 높음",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787aa644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_questions = 10\n",
    "num_contexts = 3\n",
    "\n",
    "top_context_indices = []\n",
    "\n",
    "for i in tqdm(range(num_questions)):\n",
    "    embed_q = get_embedding(rag_data['questions'][i])\n",
    "\n",
    "    similarities = []\n",
    "    for j in range(num_contexts):\n",
    "        embed_c = get_embedding(rag_data['contexts'][i][j])\n",
    "        similarity = cosine_similarity(embed_q, embed_c)\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    top_context_index = similarities.index(max(similarities))\n",
    "    top_context_indices.append(top_context_index)\n",
    "\n",
    "print(f\"Top Context Indices: {top_context_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w9mu8zetajl",
   "source": "## 🔄 10개 질문에 대한 자동 평가\n\n처음 10개의 질문에 대해 자동으로 가장 유사한 컨텍스트를 찾습니다.\n\n### 동작 과정\n1. **반복문 실행**: 10개 질문을 하나씩 처리\n2. **각 질문마다**:\n   - 질문을 임베딩으로 변환\n   - 3개의 컨텍스트를 각각 임베딩으로 변환\n   - 유사도 계산\n   - 가장 높은 유사도를 가진 컨텍스트의 인덱스 저장\n3. **진행 상황 표시**: `tqdm`으로 진행률 바 출력\n\n### 결과\n- **`top_context_indices`**: 각 질문마다 예측한 정답 컨텍스트의 위치 리스트\n- **예시**: `[2, 1, 0, 2, 1, ...]` → 첫 번째 질문은 3번째 컨텍스트가 정답이라 예측",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_data['contexts_answer_idx'][:num_questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7iphl14dxw",
   "source": "## 📋 실제 정답 인덱스 확인\n\n10개 질문의 실제 정답 컨텍스트 위치를 출력합니다.\n\n- **`contexts_answer_idx[:10]`**: 처음 10개 질문의 정답 위치 리스트\n- **비교 대상**: 위에서 예측한 `top_context_indices`와 비교할 기준값\n- **목적**: 예측이 얼마나 정확한지 평가하기 위한 정답 데이터",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7256bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predicted, actual):\n",
    "    correct = sum(p == a for p, a in zip(predicted, actual))\n",
    "    total = len(predicted)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "accuracy = calculate_accuracy(top_context_indices, rag_data['contexts_answer_idx'][:10])\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6hsa66i94kx",
   "source": "## 🎯 정확도 계산\n\n예측 결과와 실제 정답을 비교하여 정확도를 계산합니다.\n\n### 함수 설명\n- **`calculate_accuracy()`**: 예측값과 실제값을 비교하여 정확도 계산\n- **동작 원리**:\n  1. 예측과 정답이 일치하는 개수 세기\n  2. 전체 개수로 나누기\n  3. 백분율로 변환\n\n### 결과 해석\n- **Accuracy: 70%**: 10개 중 7개를 정확하게 예측했다는 의미\n- **높을수록 좋음**: 100%에 가까울수록 RAG 시스템이 정확함\n- **실전 의미**: 사용자 질문에 올바른 정보를 제공할 확률",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b05e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_predictions = []\n",
    "for i in range(len(top_context_indices)):\n",
    "    index = top_context_indices[i]\n",
    "    contexts_predictions.append([rag_data['contexts'][i][index]])\n",
    "contexts_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4lu26sglzgi",
   "source": "## 📦 예측된 컨텍스트 추출\n\n예측한 인덱스를 사용하여 실제 컨텍스트 텍스트를 추출합니다.\n\n### 동작 과정\n- **반복문**: 10개 질문의 예측 결과를 순회\n- **인덱스로 접근**: 각 질문에서 예측한 위치의 컨텍스트 가져오기\n- **리스트로 감싸기**: `[context]` 형태로 변환 (RAGAS 평가 형식 맞추기)\n\n### 결과\n- **`contexts_predictions`**: 각 질문마다 예측한 컨텍스트 텍스트가 담긴 리스트\n- **형식**: `[[context1], [context2], ...]`\n- **목적**: 다음 단계에서 RAGAS로 평가하기 위한 데이터 준비",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6780c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "import os\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness, context_relevancy, context_recall, context_precision\n",
    "\n",
    "\n",
    "data_samples = {\n",
    "    'question': rag_data['questions'][:num_questions],\n",
    "    'contexts' : contexts_predictions,\n",
    "    'ground_truth': rag_data['answers'][:num_questions]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "score = evaluate(dataset, metrics=[context_recall])\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fm0f0q41pgs",
   "source": "## 📈 RAGAS 평가 수행\n\nRAGAS 프레임워크를 사용하여 RAG 시스템의 성능을 전문적으로 평가합니다.\n\n### RAGAS란?\n- **RAG Assessment**: RAG 시스템을 평가하는 전문 라이브러리\n- **다양한 메트릭 제공**: faithfulness, relevancy, recall, precision 등\n\n### 평가 데이터 준비\n- **`question`**: 사용자 질문\n- **`contexts`**: 예측한 컨텍스트 (우리가 찾은 문단)\n- **`ground_truth`**: 실제 정답\n\n### 평가 메트릭\n- **`context_recall`**: 정답을 포함하는 컨텍스트를 얼마나 잘 찾았는가\n  - 1.0에 가까울수록 좋음\n  - 예: 0.8 → 정답 정보의 80%를 찾아냄\n\n### 결과\n평가 점수와 함께 RAG 시스템의 검색 성능이 수치로 표시됩니다.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}